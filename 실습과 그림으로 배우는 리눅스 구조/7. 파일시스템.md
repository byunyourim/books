리눅스에서는 커널은 저장 장치 안에 있는 데이터에 접근할 때 직접 저장 장치에 접근하는 대신, **파일 시스템을 통해서 접근**한다.  


## 저장 장치의 기능
특정 주소에 대해 지정된 크기의 데이터를 읽고 쓰는 방식으로 동작하며, 데이터를 영구적으로 보관하고 필요할 때 읽거나 저장한다.


![저장장치의기능.png](https://github.com/user-attachments/assets/bffcc19d-42d0-4521-9412-48e6a586dbbd)
- 100 기가바이트의 스토리지가 있다.
- 10 기가바이트의 메모리 영역을 주소 50기가바이트 지점에 쓴다

![저장장치의기능.png](https://github.com/user-attachments/assets/a63a3f97-b2c7-42b2-b6a5-dafa4dc05c8b)
- 파일시스템 없이 데이터를 읽어오는 경우, 데이터를 보관한 주소, 사이즈를 스스로 기억해야 한다.
- 따라서, 모든 데이터에 대해 주소, 사이즈를 관리해야 한다.

<br>
파일 시스템이 없으면 데이터를 찾거나 관리하는 것이 매우 어려워지며, 모든 파일과 데이터의 물리적 위치를 수동으로 관리해야 한다.


<br>
이런 것을 관리하는 방법이 파일 시스템이다  
데이터의 이름, 위치, 사이즈 등의 부가 정보를 파일 단위로 관리한다.
따라서 데이터(파일)의 이름을 알면 저장 장치에서의 데이터 위치, 사이즈 등의 정보를 기억할 필요가 없다.

파일 시스템이 있는 경우, 파일 이름이나 경로를 통해 데이터에 쉽게 접근할 수 있게 됩니다.

<br><br>

#### 단순한 파일 시스템의 사양
- 0기가 바이트의 지점부터 파일 리스트를 기록한다.
- 하나의 파일에 대해 이름, 장소, 사이즈 세 가지 정보를 기록한다.

사용자 프로세스가 파일 읽기 시스템 콜을 사용하여 파일 이름, 오프셋, 사이즈를 지정하면 파일시스템을 다루는 처리가 해당하는 데이터를 찾아 사용자에게 전달한다.
<img width="915" alt="스크린샷 2024-09-14 오전 12 58 52" src="https://github.com/user-attachments/assets/48efe4a2-59a2-4a77-83dc-c0ea8f862787">



<br><br>

## 리눅스의 파일 시스템
- 디렉터리 : 파일을 보관하는 특수 파일로, 일반 파일 또는 다른 디렉터리 보관 가능하며 트리 구조이다.
<img width="903" alt="스크린샷 2024-09-14 오전 1 15 14" src="https://github.com/user-attachments/assets/18c8eb76-21af-4fba-a680-543d8b5b0a1d">

- 리눅스는 여러 개의 파일 시스템을 다룰 수 있다.
- 각각 저장 장치의 데이터 구조 및 처리하기 위한 프로그램이 다르다.
- 각각의 파일 시스템은 다룰 수 있는 파일, 파일시스템의 사이즈, 처리 속도 등이 다르다.
- 하지만 아래의 시스템 콜을 통해 통일된 인터페이스로 접근할 수 있다.


|시스템  콜	| 동작|
|------|------|
|creat(), unlink()	|파일 생성, 삭제|
|open(), close()|	파일 열기, 닫기|
|read()	|파일로부터 데이터 읽기|
|write()	|파일에 데이터 쓰기|
|lseek()	|파일의 특정 위치로 이동|
|ioctl()|	파일시스템에 의존적인 특수한 처리|


1. 커널 내의 모든 파일시스템 공통 처리가 동작하고 대상 파일의 파일시스템을 판별한다.
2. 각 파일시스템을 처리하는 프로세스를 호출하여 시스템 콜에 대응되는 처리를 한다.
3. 데이터의 읽기를 하는 경우 디바이스 드라이어에 처리를 의뢰한다.
4. 디바이스 드라이버가 데이터를 읽어 들인다.

<img width="787" alt="스크린샷 2024-09-14 오전 1 29 37" src="https://github.com/user-attachments/assets/e826334b-ad81-4396-bddc-df9fc82dc8cf">

파일 시스템에 관계없이 통일된 인터페이스로 접근이 가능하다.



<br><br><br>


## 데이터와 메타데이터
- **데이터** : 사용자가 작성한 문서, 사진, 동영상, 프로그램 등
- **메타 데이터** : 파일의 이름이나 저장 장치 내에 위치 사이즈 등 보조 정보 (종류, 시간정보, 권한정보)


메타데이터는 데이터와 비교해서 사이즈가 작아 큰 용량을 차지하지는 않는다. 하지만 작은 파일을 많이 작성하는 시스템의 경우,
데이터의 총 용량을 측정해보면 실제로 디스크 총 사용량에 비해 상대적으로 적은 용량을 차지하는 경우가 있다.
이러한 경우가 메타데이터가 데이터보다 많은 용량을 차지하는 경우이다.  

<br>

#### 명령어
- df : 파일 시스템의 스토리지 사용량(파일 시스템에 작성하나 모든 파일의 합계 + 메타데이터 사이즈)









<br><br><br>

## 용량 제한
시스템을 여러 가지 용도로 사용하는 경우
특정 용도가 파일 시스템의 용량을 무제한으로 사용할 수 있다면, 다른 용도로 사용할 용량이 부족해진다.
<img width="1012" alt="스크린샷 2024-09-14 오전 2 00 11" src="https://github.com/user-attachments/assets/0df810b1-4143-4b14-a6fd-815a76448953">

#### 쿼터의 종류
- 사용자 쿼터 : 사용자별로 용량 제한, (ext4, XFS)
- 디렉터리 쿼터 : 프로젝트 쿼터라고도 하며, 특정 디렉터리별로 용량을 제한, (ext4, XFS)
- 서브 볼륨 쿼터 : 파일 시스템 내의 서브 볼륨이라는 단위별 용량 제한,(Btrfs)








<br><br><br>

## 파일 시스템이 깨진 경우
파일시스템의 데이터를 스토리지로 쓰는 중에 시스템의 전원이 강제로 끊어지는 경우 파일 시스템의 내용이 깨질 수 있다.  

디렉터리의 이동을 예로 보자.
<img width="976" alt="스크린샷 2024-09-14 오전 2 27 57" src="https://github.com/user-attachments/assets/cb1dc16e-580d-4111-b8af-b28476e8d9ee">


<br>


### 이동 처리의 흐름을 보자
<img width="970" alt="스크린샷 2024-09-14 오전 2 27 50" src="https://github.com/user-attachments/assets/e1d24169-b549-4a6c-ad59-79306a050bb2">


<br>


### 파일시스템이 깨진 경우
<img width="981" alt="스크린샷 2024-09-14 오전 2 27 39" src="https://github.com/user-attachments/assets/cb38b50b-82dc-4e19-9ca6-33973377931e">

파일시스템이 깨지는 것을 막기 위해서는 **저널링과 Copy on Write** 방식이 있다.
- 저널링 : ext4, XFS
- Copy on Write : Btrfs


<br><br><br>

## 저널링
저널링에서는 파일시스템 안에 저널 영역이라는 특수한 영역이 있다.
저널 영역은 사용자가 인식할 수 없는 메타데이터이다.  


#### 파일 시스템을 업데이터하는 순서
1. 업데이트에 필요한 아토믹한 처리의 목록을 저널 영역에 작성한다. 이 목록을 저널 로그라고 한다.
2. 저널 영역의 내용을 바탕으로 파일 시스템의 내용을 업데이트한다.

<img width="989" alt="스크린샷 2024-09-14 오전 2 50 41" src="https://github.com/user-attachments/assets/0debda14-345d-45cc-9539-43c171d92ad5">

<br>

#### 순서 2에서 전원이 끊어진다면?
<img width="1009" alt="스크린샷 2024-09-14 오전 2 55 05" src="https://github.com/user-attachments/assets/8cb87a72-dac9-4c63-822a-4ecc102723f5">
단순히 저널 영역의 데이터를 지워버린다. 따라서 실제 데이터는 처리 전과 같아진다.

<br>

#### 하지만 실제로 데이터를 업데이트하는 중에 강제로 전원이 끊어진다면?
저널로그를 처음부터 다시 수행한다.


<br><br><br>

## Copy on Write
파일시스템이 깨지는 것 방지  

예전부터 있던 파일시스템은 일단 파일을 작성하면 그 파일의 배치 장소는 바뀌지 않는다. 
파일의 내용을 업데이트할 때마다 저장 장치상의 같은 장소에 새로운 데이터를 넣는다.
<img width="1020" alt="스크린샷 2024-09-14 오전 3 12 30" src="https://github.com/user-attachments/assets/f99efe6e-54f9-4c3d-93a5-305a2ada4a49">


<br>

Btrfs 등의 Copy on Write 형의 파일시스템은 파일을 작성하더라도 업데이트할 때와 다른 장소에 데이터를 기록한다.  
<img width="1024" alt="스크린샷 2024-09-16 오전 12 12 34" src="https://github.com/user-attachments/assets/173b53fb-ae2a-4a8a-af60-49a7f8e08d49">

<br>

<img width="1011" alt="스크린샷 2024-09-16 오전 12 13 46" src="https://github.com/user-attachments/assets/62cb1735-211c-4204-bbef-0da567a5d2e2">
업데이트되는 데이트를 다른 장소에 전부 쓴 뒤 링크를 고치는 방식으로 동작한다.


<br>
<img width="986" alt="스크린샷 2024-09-16 오전 12 19 52" src="https://github.com/user-attachments/assets/683acbb4-a093-4221-a9d3-e1bf077d9580">

위와 같이 강제 전원 단절이 발생해도 재부팅 후에 작성된 데이터를 삭제하면 문제가 없다.

<br><br><br>


## 대책
마지막에 백업한 시점의 상태로 복원  

복구용 명령어
- fsck

**단점**
- 파일시스템 전체를 조사하기 때문에 소요 시간이 파일시스템 사용량에 따라 증가한다. (수테라바이트 ~ 수일 단위)
- 복구에 오랜 시간을 들여도 실패할 수 있다.
- 사용자가 원하는대로 상태를 복원한다고 보장할 수 없다.
- 처리하면서 깨진 데이터는 내용과 관계없이 삭제한다.

<br>

#### fsck의 동작
<img width="986" alt="스크린샷 2024-09-16 오전 12 19 52" src="https://github.com/user-attachments/assets/b2236c0d-9bde-4704-a356-c5a502eec639">


<br><br><br>

## 파일 종류
- 일반 파일 : 사용자 데이터를 보관
- 디렉터리 : 파일을 보관
- 디바이스 파일 :

  리눅스에서는 스스로 동작하는 하드웨어 상의 장치를 파일로 표현한다. 그래서 리눅스에서는 장치를 파일과 동등하게 open(), read(), write()등의 시스템 콜을 사용한다.
  장치 고유의 복잡한 조작에는 ioctl()시스템 콜을 사용한다.

  root만 디바이스 파일에 접근이 가능하다.


캐릭터 장치와 블록 장치가 있다.
각 디바이스 파일은 /dev 아ㅏ래에 존재한다. 디바이스 파일의 메타뎅터에 보관되어 있는 정보에 따라 각각의 장치를 식별한다.

- 파일의 종류 (캐릭터 장치, 블록 장치)
- 장치의 Major number
- 장치의 Minor number


<br><br><br>

## 캐릭터 장치
읽기와 쓰기는 가능하나 탐색 불가
- 터미널 : bash등의 셸을 통해 명령어를 실행하기 위해 문자열만으로 이루어진 흑백 화면 혹은 윈도우
  - write() 시스템 콜 : 터미널에 데이터 출력
  - read() 시스템 콜 : 터미널에 데이터 입력
- 키보드
- 마우스



<br><br><br>

## 블록 장치
- 블록의 읽기, 쓰기, 랜덤 접근 가능
- HDD, SDD 등의 저장 장치
- 블록 장치에 데이터를 읽고 쓴다.
- 스토리지의 특정 장소에 있ㅅ는 데이터에 접근 가능
- 일반적으로 직접 접근하지 않고, 파일시스템을 작성하여 마운트함으로써 파일시스템을 경유해서 사용


### 블록장치를 직접 다루는 경우
- 파티션 테이블의 업데이트(parted)
- 블록 장치 레벨의 데이터 백업 & 복구(dd)
- 파일시스템의 작성(mkfs)
- 파일시스템의 마운트(mount)
- fsck







<br><br><br>

## 여러가지 파일시스템




<br><br><br>

### 메모리 기반
#### tmpfs
- 저장 장치 대신 메모리에 작성하는 파일시스템
- 전원 종료시 데이터가 사라지지만 저장 장치의 접근이 전혀 발생하지 않음
- 고속
<img width="995" alt="스크린샷 2024-09-15 오후 4 29 11" src="https://github.com/user-attachments/assets/9f9b1291-b53f-4261-ad71-b5145982c747">


<br><br><br>

### 네트워크 파일시스템
지금까지의 파일시스템은 로컬 시스템에 있는 데이터  
네트워크 파일시스템은 네트워크를 통해 연결된 원격 호스트에 있는 파일에 접근한다.  
<img width="1016" alt="스크린샷 2024-09-15 오후 4 34 50" src="https://github.com/user-attachments/assets/0046dff7-fcf3-4b56-89c6-1c1cd5fbe617">

- cifs : windows 호스트상의 파일에 접근
- nfs : UNIX 계열의 OS를 사용하는 호스트 상의 파일에 접근



<br><br><br>

### 가상 파일시스템
####procfs : 시스템에 존재하는 프로세스에 대한 정보를 얻기 위한 파일시스템

#### sysfs
- procfs가 도입된 후 시간이 지남에 따라 커널의 프로세스 정보 외 여러 정보가 제한 없이 들어감. procfs를 마구잡이로 사용하는 것을 막기 위해, 이러한 정보를 배치하는 장소를 정하기 위해 만든 파일시스템
- /sys이하에 마운트
- /sys/devices : 시스템에 탑재된 디바이스에 대한 정보
- /sys/fs 이하의 파일 : 시스템에 존재하는 각종 파일시스템에 대한 정보

#### cgroupfs
- 하나의 프로세스 혹은 여러 개의 프로세스로 만들어진 그룹에 대해 여러 리소스 사용량의 제한을 가하는 cgroup기능을 갖는다.
- root만 cgroup 기능 사용 가능
- /sys/fs/cgroup 이하에 마운트


<br><br><br>

### Btrfs

<br>

#### 멀티볼륨
<img width="1013" alt="스크린샷 2024-09-15 오후 4 41 16" src="https://github.com/user-attachments/assets/05f29093-0a06-4321-b7d5-bdd94e6a9c3e">

<img width="1005" alt="스크린샷 2024-09-15 오후 4 50 19" src="https://github.com/user-attachments/assets/12f5ccd6-fb92-40a0-8297-fe4e0aae4f4a">

여러 개의 저장 장치/파티션으로부터 거대한 스토리지 풀을 만들고 그 안에 마운트 가능한 서브 볼륨 영역을 작성
- 스토리지 풀 = LVM으로 구현된 볼륨 그룹
- 서브 볼륨 = LVM으로 구현된 논리 볼륨 + 파일시스템
- 파일시스템 + LVM


<br>


#### 스냅샷

#### 일반 복사
<img width="1017" alt="스크린샷 2024-09-15 오후 4 58 53" src="https://github.com/user-attachments/assets/e05960f2-ac79-4cb3-bc89-00f10cd273e9">

#### 스냅샷
<img width="1022" alt="스크린샷 2024-09-15 오후 5 05 21" src="https://github.com/user-attachments/assets/7cb5b0cd-7dce-46ec-a93d-ec4ef2573a45">

서브 볼륨 단위로 스냅샷 생성 가능
- 데이터를 참조하는 메타데이터 작성, 스냅샷 내 더티 페이지의 라이트백 -> 일반적인 복사보다 빠르다
- 원래의 서브 볼륨과 스냅샷은 데이터를 공유하기때문에 공간낭비가 적다

공산상 비용의 차이



이 경우 트리의 root 노드만을 새로 만들어 다음 레벨의 노드를 링크건다.
즉, 데이터의 복사가 발생하지 않기때문에 훨씬 고속이다.


<br>


#### RAID
파일시스템 레벨에 RAID 작성을 포함한다.  
RAID 0, 1, 10, 5, 6, dup(이중화)  
- 어느 레벨로 설정되는지의 단위는 서브볼륨이 아닌 Btrfs 파일시스템 전체



<br>


#### RAID가 없는 경우 sda가 망가지는 경우
<img width="999" alt="스크린샷 2024-09-15 오후 5 11 50" src="https://github.com/user-attachments/assets/13a434bc-75e6-47ec-83f8-6d3717b74e9b">



<br>


#### RAID 1 구성인 경우 sda가 망가지는 경우
<img width="981" alt="스크린샷 2024-09-15 오후 5 22 08" src="https://github.com/user-attachments/assets/4bb446e3-e4ef-41f3-9403-73b2ee028c59">

모든 데이터는 2개의 스토리(sda, sdb)에 저장되기 때문에 sda가 망가지더라도 A의 데이터는 sdb에 남아 있다.


<br>

#### 데이터의 파손 검출, 복구

스토리지 내의 일부 데이터가 파괴된 경우 RAID 구성을 이용해 복구가 가능하다.  

<img width="1018" alt="스크린샷 2024-09-16 오전 12 04 07" src="https://github.com/user-attachments/assets/f8b6098e-ebbc-4787-8088-00048fe2b861">
- 데이터가 깨진 것을 검출하지 못하면 모른 채로 운영을 계속하게 되는 리스크가 있다.
- Brtfs의 경우 데이터, 메타데이터 모두 일정 데이터 크기마다 체크섬을 가지고 있으므로 데이터의 파손을 검출할 수 있다.

<br>

<img width="989" alt="스크린샷 2024-09-16 오전 12 04 56" src="https://github.com/user-attachments/assets/b12af234-dbb6-4d55-b141-686b26886126">

- 데이터를 읽는 도중 체크섬 에러를 검출하게 되면 해당 데이터를 버리고 읽기를 요청한 프로그램에 I/O 에러를 알린다.
- RAID 1, 10, 5, 6,dup 구성인 경우 다른 체크섬이 일치하는 정확한 데이터를 기준으로 데이터를 복구한다.
- RAID 5, 6 경우 패리티 사용




RAID 1의 경우 읽기를 요청한 곳은 데이터가 일시적으로 깨졌음을 인식하지 못하고 지나간다.


